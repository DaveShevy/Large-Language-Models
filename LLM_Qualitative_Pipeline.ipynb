{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ---------- Import Libraries/Packages ---------- ##\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import openai\n",
    "import time\n",
    "import numpy as np\n",
    "import ast\n",
    "from rouge_score import rouge_scorer\n",
    "from dotenv import load_dotenv\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from transformers import pipeline\n",
    "from openai import AzureOpenAI\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "## ---------- Load Data and Define Standard Variables ---------- ##\n",
    "\n",
    "# These variables are client-specific and will change to match individual clients' data\n",
    "\n",
    "data_folder = r\"C:\\Users\\DavidShevchenko\\Downloads\"\n",
    "excel_file_path = os.path.join(data_folder, '.xlsx')\n",
    "\n",
    "## ---------- Define Roles ---------- ##\n",
    "\n",
    "# Defining the roles that the LLM will play when creating key themes and text summaries\n",
    "\n",
    "key_themes_role = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You are an AI assistant that extracts key themes from survey comments. Accuracy is incredibly important. Themes will be used to assess agency/client relationships.\"\n",
    "}\n",
    "key_themes_prompt = (\n",
    "    \"You will be provided with a survey comment from an Agency/Novartis relationship evaluation. \"\n",
    "    \"Your task is to extract 1-3 key themes from the comment. \"\n",
    "    \"Make sure the themes are relevant to the content of the comment. \"\n",
    "    \"Separate the themes with a comma and ENSURE that each theme is 1-2 words maximum. \"\n",
    "    \"If no key themes can be identified, please respond with 'No Key Themes'. \"\n",
    "    \"Example input: 'The agency's response time was very quick and they were extremely knowledgeable about our needs.' \"\n",
    "    \"Example output: 'Responsive, Knowledgeable'\"\n",
    ")\n",
    "\n",
    "## ---------- Load Environment Variables and Test ---------- ##\n",
    "\n",
    "# Load environment variables from a .env file into the system's environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve Azure service credentials and configurations from environment variables\n",
    "AZURE_OPENAI_API_KEY = os.getenv('AZURE_OPENAI_API_KEY')\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv('AZURE_OPENAI_ENDPOINT')\n",
    "AZURE_OPENAI_DEPLOYMENT = os.getenv('AZURE_OPENAI_DEPLOYMENT_NAME')\n",
    "AZURE_OPENAI_EMBEDDING_DEPLOYMENT = os.getenv('AZURE_OPENAI_EMBEDDING_NAME')\n",
    "API_VERSION = os.getenv('AZURE_OPENAI_MODEL_VERSION')\n",
    "AZURE_LANGUAGE_ENDPOINT = os.getenv('AZURE_LANGUAGE_ENDPOINT')\n",
    "AZURE_LANGUAGE_KEY = os.getenv('AZURE_LANGUAGE_API_KEY')\n",
    "\n",
    "# Print statement confirming all credentials are accounted for\n",
    "print(\n",
    "    \"AZURE_OPENAI_API_KEY:\", AZURE_OPENAI_API_KEY is not None,\n",
    "    \"\\nAZURE_OPENAI_ENDPOINT:\", AZURE_OPENAI_ENDPOINT is not None,\n",
    "    \"\\nAZURE_OPENAI_DEPLOYMENT:\", AZURE_OPENAI_DEPLOYMENT is not None,\n",
    "    \"\\nAZURE_OPENAI_EMBEDDING_DEPLOYMENT:\", AZURE_OPENAI_EMBEDDING_DEPLOYMENT is not None,\n",
    "    \"\\nAPI_VERSION:\", API_VERSION is not None,\n",
    "    \"\\nAZURE_LANGUAGE_ENDPOINT:\", AZURE_LANGUAGE_ENDPOINT is not None,\n",
    "    \"\\nAZURE_LANGUAGE_KEY:\", AZURE_LANGUAGE_KEY is not None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ---------- Authenticate Azure Client, and Redact PII ---------- ##\n",
    "\n",
    "# Authenticates and initializes Azure Text Analytics client\n",
    "def authenticate_client(key, endpoint):\n",
    "    ta_credential = AzureKeyCredential(key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "        endpoint=endpoint,\n",
    "        credential=ta_credential\n",
    "    )\n",
    "    return text_analytics_client\n",
    "\n",
    "client = authenticate_client(AZURE_LANGUAGE_KEY, AZURE_LANGUAGE_ENDPOINT)\n",
    "\n",
    "# Use Azure's Text Analytics SDK to recognize and remove PII entities in the text.\n",
    "# This PII redacted text will be passed into the LLM in order to avoid breach of PII\n",
    "def redact_commentary_pii(df, client):\n",
    "    def redact_pii_with_sdk(text, client):\n",
    "        try:\n",
    "            response = client.recognize_pii_entities(documents=[text], language=\"en\")[0]\n",
    "            if not response.is_error:\n",
    "                for entity in response.entities:\n",
    "                    # Exclude certain categories from redaction\n",
    "                    if entity.category.lower() not in [\"persontype\", \"organization\"]: # Include Datetime as exclusion\n",
    "                        text = text.replace(entity.text, f\"[{entity.category.lower()}]\")\n",
    "                return text\n",
    "            else:\n",
    "                print(f\"Error in response: {response.error}\")\n",
    "                return text\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during PII redaction: {e}\")\n",
    "            return text\n",
    "\n",
    "    # Apply the redact_commentary_pii helper function to dataframe creating new column 'pii_redact'\n",
    "    df['pii_redact'] = df['comment'].apply(lambda text: redact_pii_with_sdk(text, client))\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ---------- Authenticate Azure Client, and Perform Sentiment Analysis ---------- ##\n",
    "\n",
    "# Initialize sentiment analysis pipeline using BERT model\n",
    "bert_sentiment = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "# Authenticates and initializes Azure Text Analytics client\n",
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(AZURE_LANGUAGE_KEY)   ####THIS FUNCTION WAS DEFINED ABOVE\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "        endpoint=AZURE_LANGUAGE_ENDPOINT,\n",
    "        credential=ta_credential\n",
    "    )\n",
    "    return text_analytics_client\n",
    "\n",
    "# Initialize the Azure client\n",
    "client = authenticate_client()\n",
    "\n",
    "# Both BERT sentiment Analysis and Azure sentiment Analysis will be performed on the text to extract a sentiment score of negative, neutral, or positive.\n",
    "# Azure's scores will be used in the report and BERT is used as a comparison for an extra level of QA\n",
    "\n",
    "def get_BERT_sentiment(text):\n",
    "    # Check if text is too long for the allowed tokens for sentiment analysis\n",
    "    if len(text) > (512 * 6):\n",
    "        # Return 1 if the comment is too long\n",
    "        return \"Comment too Long\", 1.0\n",
    "    try:\n",
    "        # Analyze the sentiment of the text using the BERT pipeline\n",
    "        result = bert_sentiment(text)\n",
    "        # return the score from the results\n",
    "        return result[0]['label'], result[0]['score']\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing comment: {e}\")\n",
    "        # Return 0 if there is an error\n",
    "        return \"Error processing comment\", 0\n",
    "\n",
    "def sentiment_analysis(client, text):\n",
    "    try:\n",
    "        documents = [{\"id\": \"1\", \"language\": \"en\", \"text\": text}]\n",
    "        # Analyze sentiment of the document using the Azure Text Analytics client and the documents defined above\n",
    "        response = client.analyze_sentiment(documents=documents, disable_service_logs=True)\n",
    "        document = response[0]\n",
    "\n",
    "        # Change the results of the sentiment analysis from \"mixed\" to neutral for consistency\n",
    "        sentiment = document.sentiment.lower()\n",
    "        if sentiment == 'mixed':\n",
    "            sentiment = 'neutral'\n",
    "\n",
    "        # Create a dictionary with the sentiment analysis results\n",
    "        sentiment_data = {\n",
    "            # Capitalize the sentiment label for consistency\n",
    "            \"Azure Sentiment\": sentiment.capitalize(),\n",
    "            # Extract the positive, neutral, and negative scores\n",
    "            \"Azure Positive Score\": document.confidence_scores.positive,\n",
    "            \"Azure Neutral Score\": document.confidence_scores.neutral,\n",
    "            \"Azure Negative Score\": document.confidence_scores.negative\n",
    "        }\n",
    "        return sentiment_data\n",
    "    except Exception as err:\n",
    "        print(f\"Encountered exception: {err}\")\n",
    "        # Return a score of 0 if an error occurs\n",
    "        return {\n",
    "            \"Azure Sentiment\": 'Error',\n",
    "            \"Azure Positive Score\": 0.0,\n",
    "            \"Azure Neutral Score\": 0.0,\n",
    "            \"Azure Negative Score\": 0.0\n",
    "        }\n",
    "\n",
    "# Create dictionary to rename the BERT output in stars to 'Negative', 'Neutral', or Positive for consistency with the Azure sentiment\n",
    "replacements = {\n",
    "    '1 star': 'Negative',\n",
    "    '2 stars': 'Negative',\n",
    "    '3 stars': 'Neutral',\n",
    "    '4 stars': 'Positive',\n",
    "    '5 stars': 'Positive',\n",
    "    'Comment too Long': 'Unknown',\n",
    "    'Error processing comment': 'Unknown'\n",
    "}\n",
    "\n",
    "# Use the replacements dictionary to create a BERT Sentiment Column from the BERT Stars column\n",
    "valid_comments_df['BERT Sentiment'] = valid_comments_df['BERT Sentiment Stars'].replace(replacements)\n",
    "# Apply the Azure sentiment Analysis function  to the Valid Commments\n",
    "results = valid_comments_df['pii_redact'].apply(lambda text: sentiment_analysis(client, text))\n",
    "# Create dataframe from the Azure sentiment analysis results\n",
    "results_df = pd.DataFrame(results.tolist(), index=valid_comments_df.index)\n",
    "# Merge the Azure sentiment analysis results with the BERT results\n",
    "valid_comments_df = pd.concat([valid_comments_df, results_df], axis=1)\n",
    "\n",
    "# Merge the sentiment analysis results with the public_ai_commentary dataframe\n",
    "public_ai_commentary = public_ai_commentary.merge(\n",
    "    valid_comments_df[[\n",
    "        'BERT Sentiment Stars', 'BERT Confidence', 'BERT Sentiment',\n",
    "        'Azure Sentiment', 'Azure Positive Score', 'Azure Neutral Score', 'Azure Negative Score'\n",
    "    ]],\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill the NA values in public_ai_commentary with specified values\n",
    "public_ai_commentary.fillna({\n",
    "    'Azure Positive Score': 0.0,\n",
    "    'Azure Neutral Score': 0.0,\n",
    "    'Azure Negative Score': 0.0,\n",
    "    'BERT Confidence': 0.0,\n",
    "    'BERT Sentiment Stars': 'Not Valid',\n",
    "    'BERT Sentiment': 'Not Valid',\n",
    "    'Azure Sentiment': 'Not Valid'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ---------- Authenticate Azure Client, Extract Key Themes utilizing Azure OpenAI ---------- ##\n",
    "\n",
    "# Initialize the Azure Client\n",
    "client = AzureOpenAI(\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    api_version=API_VERSION,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT\n",
    ")\n",
    "\n",
    "# Function to extract key themes from the comments\n",
    "def extract_key_themes(df: pd.DataFrame, prompt: str, key_themes_role: dict) -> pd.DataFrame:\n",
    "    # Initializes a column 'azure_key_themes' and populates with 'Not Valid'\n",
    "    df['azure_key_themes'] = 'Not Valid'\n",
    "\n",
    "    # Returns a list of indices for all of the valide comments    ### MAY BE REDUNDANT AS WE HAVE ALREADY FILTERED THE DATAFRAME FOR VALID COMMENTS\n",
    "    valid_indices = df[df['valid'] == 'yes'].index\n",
    "\n",
    "    # Determine which rows to exclude\n",
    "    for idx in valid_indices:\n",
    "        exclude_row = False\n",
    "        if columns_to_exclude_rows:\n",
    "            for col in columns_to_exclude_rows:\n",
    "                if col in df.columns and df.at[idx, col] in exclude_rows:\n",
    "                    exclude_row = True\n",
    "                    break\n",
    "        # If the comment is invalid replace the Azure_key_themes value with pii_redact\n",
    "        if exclude_row:\n",
    "            df.at[idx, 'azure_key_themes'] = df.at[idx, 'pii_redact']\n",
    "        else:\n",
    "            # extract comment from pii_redact if row is valid\n",
    "            comment = df.at[idx, 'pii_redact']\n",
    "            try:\n",
    "                # Generate Chat completion from specified rolls and prompts\n",
    "                response = client.chat.completions.create(\n",
    "                    model=AZURE_OPENAI_DEPLOYMENT,\n",
    "                    messages=[\n",
    "                        # System role that was created earlier\n",
    "                        key_themes_role,\n",
    "                        # User role with predefined prompt\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": f\"{prompt}\\n\\n{comment}\"\n",
    "                        }\n",
    "                    ],\n",
    "                    max_tokens=30,\n",
    "                    temperature=0\n",
    "                )\n",
    "                theme = response.choices[0].message.content.strip()\n",
    "                # replace values in 'azure_key_themes' with AI generated themes\n",
    "                df.at[idx, 'azure_key_themes'] = theme\n",
    "            except Exception as e:\n",
    "                df.at[idx, 'azure_key_themes'] = f\"Error processing: {e}\"\n",
    "\n",
    "    return df\n",
    "\n",
    "# Use extract_key_themes function to populate public_ai commentary with Azure OpenAI's key themes\n",
    "public_ai_commentary = extract_key_themes(public_ai_commentary, key_themes_prompt, key_themes_role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ---------- Vectorize Each Comment and Each Theme ---------- ##\n",
    "\n",
    "# Assign API key, endpoint, and version\n",
    "openai.api_key = AZURE_OPENAI_API_KEY\n",
    "openai.api_base = AZURE_OPENAI_ENDPOINT\n",
    "openai.api_version = API_VERSION\n",
    "\n",
    "# Embed the comments in batches of 100 to avoid API rate limits\n",
    "def batch_get_embeddings(texts, deployment_id, batch_size=100):\n",
    "    embeddings = []\n",
    "    # Split the text into batches of 100\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        # Extract the batch of 100 that the loop is currently on\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        success = False\n",
    "        while not success:\n",
    "            # Use OpenAI embeddings to embed the batch of 100 comments\n",
    "            try:\n",
    "                response = openai.embeddings.create(\n",
    "                    model=deployment_id,\n",
    "                    input=batch_texts\n",
    "                )\n",
    "\n",
    "                batch_embeddings = [data.embedding for data in response.data]\n",
    "                embeddings.extend(batch_embeddings)\n",
    "                success = True\n",
    "            # If there is an error sleep for 5 minutes and then try again\n",
    "            except openai.OpenAIError as e:\n",
    "                print(f\"Service request error: {e}\")\n",
    "                time.sleep(5)\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "                time.sleep(5)\n",
    "    return embeddings\n",
    "\n",
    "# Extract the comments as a list\n",
    "pii_texts = public_ai_commentary['pii_redact'].tolist()\n",
    "# Run batch_embeddings function on the list of comments\n",
    "pii_embeddings = batch_get_embeddings(pii_texts, deployment_id=AZURE_OPENAI_EMBEDDING_DEPLOYMENT)\n",
    "# Add the embeddings to public_ai_commentary as the column 'pii_redacted_embedding'\n",
    "public_ai_commentary['pii_redact_embedding'] = pii_embeddings\n",
    "\n",
    "# Repeat the process with the key themes to get the embeddings for the 'key_themes_embedding' column\n",
    "key_theme_texts = public_ai_commentary['azure_key_themes'].tolist()\n",
    "key_theme_embeddings = batch_get_embeddings(key_theme_texts, deployment_id=AZURE_OPENAI_EMBEDDING_DEPLOYMENT)\n",
    "public_ai_commentary['key_themes_embedding'] = key_theme_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ---------- Perform PCA on Each Comment and Theme Embedding ---------- ##\n",
    "\n",
    "# PCA is used to convert the embeddings into 2D so they can be plotted in the QA report\n",
    "\n",
    "# Convert the embeddings to numpy arrays\n",
    "def convert_embedding(embedding):\n",
    "    if isinstance(embedding, str):\n",
    "        return np.array(ast.literal_eval(embedding))\n",
    "    else:\n",
    "        return np.array(embedding)\n",
    "\n",
    "\n",
    "# Function to reduce the embeddings to their 2 components that retain the most variability from original embeddings\n",
    "def apply_pca_to_embeddings(df, embedding_column, n_components=2, prefix='embedding_pca'):\n",
    "    df[embedding_column] = df[embedding_column].apply(convert_embedding)\n",
    "\n",
    "    embeddings_array = np.vstack(df[embedding_column].values)\n",
    "\n",
    "    pca = PCA(n_components=n_components)\n",
    "    reduced_embeddings = pca.fit_transform(embeddings_array)\n",
    "\n",
    "    for i in range(n_components):\n",
    "        df[f'{prefix}{i+1}'] = reduced_embeddings[:, i]\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply PCA to 'pii_redact_embedding'\n",
    "public_ai_commentary = apply_pca_to_embeddings(\n",
    "    df=public_ai_commentary,\n",
    "    embedding_column='pii_redact_embedding',\n",
    "    n_components=2,\n",
    "    prefix='pii_embedding_pca'\n",
    ")\n",
    "\n",
    "# Apply PCA to 'key_themes_embedding'\n",
    "public_ai_commentary = apply_pca_to_embeddings(\n",
    "    df=public_ai_commentary,\n",
    "    embedding_column='key_themes_embedding',\n",
    "    n_components=2,\n",
    "    prefix='key_embedding_pca'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ---------- Develop summaries and public_ai_summaries table ---------- ##\n",
    "\n",
    "# Initialize the Azure client\n",
    "client = AzureOpenAI(\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    api_version=API_VERSION,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT\n",
    ")\n",
    "\n",
    "\n",
    "def generate_grouped_summaries(df: pd.DataFrame, group_by_column: str, text_column: str, prompt_template: str, placeholder_columns: dict, system_prompt: dict, output_columns: list, summary_column_name: str, prompt_column_name: str) -> pd.DataFrame:\n",
    "    # Initialze results list that will hold the summaries after they are generated\n",
    "    results = []\n",
    "\n",
    "    # Group the dataframe by the groupby column - this was variable was defined in the first cell as \"summarize_by_column\"\n",
    "    grouped_df = df.groupby(group_by_column)\n",
    "\n",
    "    # Iterate over each group in the dataframe\n",
    "    for group_name, group in grouped_df:\n",
    "        # Create a new dictionary to store the summary information for each group\n",
    "        row = {}\n",
    "        row[group_by_column] = group_name\n",
    "\n",
    "        # Iterate over the output columns which was defined in the first cell\n",
    "        for col in output_columns:\n",
    "            if col in group.columns:\n",
    "                # collect all of the unique values in the column\n",
    "                unique_values = group[col].dropna().unique()\n",
    "                # If there is only one unique value, assign it to the dictionary\n",
    "                if len(unique_values) == 1:\n",
    "                    row[col] = unique_values[0]\n",
    "                # If there are multiple unique values, seperate them by a comma and then assign them to the dictionary\n",
    "                else:\n",
    "                    row[col] = ', '.join(map(str, unique_values))\n",
    "            else:\n",
    "                row[col] = None\n",
    "\n",
    "        # Prompt_template was defined in the first cell and consists of unique prompts for each type of summary\n",
    "        prompt = prompt_template\n",
    "\n",
    "\n",
    "        for placeholder, col_name in placeholder_columns.items():\n",
    "            if col_name in group.columns:\n",
    "                # Extract the unique values from col_name that was defined by placeholder_columns\n",
    "                unique_values = group[col_name].dropna().unique()\n",
    "                # If there is only one unique value use that value\n",
    "                if len(unique_values) == 1:\n",
    "                    value = unique_values[0]\n",
    "                # If there are multiple unique values seperate them by a comma and use those values\n",
    "                else:\n",
    "                    value = ', '.join(map(str, unique_values))\n",
    "                # In each prompt, replace the placeholder text with the unique values\n",
    "                prompt = prompt.replace(placeholder, str(value))\n",
    "            else:\n",
    "                prompt = prompt.replace(placeholder, '')\n",
    "\n",
    "        # Extract all of the comments from the grouped dataframe as a list - in this case they are the PII redacted comments\n",
    "        comments = group[text_column].dropna().tolist()\n",
    "        # Convert the comments into a string split by \"|\"\n",
    "        comments_text = ' | '.join(comments)\n",
    "\n",
    "        # Define the comment count name and fill with the length of comments to get the total count\n",
    "        comment_count_column_name = f'{summary_column_name}_comment_count'\n",
    "        row[comment_count_column_name] = len(comments)\n",
    "\n",
    "        # Add the comment string to the prompt to create the final prompt and add that to the dataframe\n",
    "        final_prompt = f\"{prompt}\\n\\n{comments_text}\"\n",
    "        row[prompt_column_name] = final_prompt\n",
    "\n",
    "        # Create the message that will be passed into the LLM. The system prompt was defined in the first cell and lets the LLM\n",
    "        # know whether it will be performin text summarization or extracting key themes\n",
    "        messages = [\n",
    "            system_prompt,\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": final_prompt\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        # Generate the Summary using Azure OpenAI\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=AZURE_OPENAI_DEPLOYMENT,\n",
    "                messages=messages,\n",
    "                max_tokens=500,\n",
    "                temperature=0\n",
    "            )\n",
    "            summary = response.choices[0].message.content.strip()\n",
    "            row[summary_column_name] = summary\n",
    "        except Exception as e:\n",
    "            row[summary_column_name] = f\"Error processing: {e}\"\n",
    "\n",
    "        # Append the summary to the results list\n",
    "        results.append(row)\n",
    "\n",
    "    # Create dataframe form the results list\n",
    "    summary_df = pd.DataFrame(results)\n",
    "\n",
    "    return summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ---------- Vectorize Each Summary ---------- ##\n",
    "\n",
    "# Function to Vectorize each summary. These vectors will be utilized for tools such as cosine similarity\n",
    "def batch_get_embeddings(texts, deployment_id, batch_size=100):\n",
    "    # Initialize a list that will hold the summary embeddings\n",
    "    embeddings = []\n",
    "    # Embed the comments in batches of 100 to avoid API rate limits\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        # Split text into the batch that the loop is currently on\n",
    "        batch_texts = [text for text in texts[i:i + batch_size] if text.strip()]\n",
    "\n",
    "        # If there is no text add None to the embeddings\n",
    "        if not batch_texts:\n",
    "            embeddings.extend([None] * batch_size)\n",
    "            continue\n",
    "\n",
    "        success = False\n",
    "        while not success:\n",
    "            try:\n",
    "                # Embed the texts with OpenAi Embeddings\n",
    "                response = openai.embeddings.create(\n",
    "                    model=deployment_id,\n",
    "                    input=batch_texts\n",
    "                )\n",
    "                # Extract the embedding form the output and add it to the embeddings list\n",
    "                batch_embeddings = [data.embedding for data in response.data]\n",
    "                embeddings.extend(batch_embeddings)\n",
    "                success = True\n",
    "            # If there is an error wait 5 seconds and try again\n",
    "            except openai.OpenAIError as e:\n",
    "                print(f\"Service request error: {e}\")\n",
    "                time.sleep(5)\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "                time.sleep(5)\n",
    "    # Initialize final embeddings list\n",
    "    final_embeddings = []\n",
    "    text_idx = 0\n",
    "\n",
    "    # Loop through the original text and append the embedding corresponding to the index of the original text\n",
    "    for text in texts:\n",
    "        if text.strip():\n",
    "            final_embeddings.append(embeddings[text_idx])\n",
    "            text_idx += 1\n",
    "        else:\n",
    "            final_embeddings.append(None)\n",
    "\n",
    "    return final_embeddings\n",
    "\n",
    "# Loop through each summary column and generate embeddings\n",
    "for summary in summaries:\n",
    "    texts = public_ai_summaries[summary].fillna(\"\").tolist()\n",
    "    embeddings = batch_get_embeddings(texts, deployment_id=AZURE_OPENAI_EMBEDDING_DEPLOYMENT)\n",
    "    public_ai_summaries[f'{summary}_embedding'] = embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ---------- METRIC: Cosine Similarity  ---------- ##\n",
    "\n",
    "# Cosine Similarity is used to compare the summary and key them vectors to eacb comment's vector.\n",
    "\n",
    "# Funtion meant to convert the embeddings to numpy arrays\n",
    "def convert_embedding(embedding):\n",
    "    if isinstance(embedding, str):\n",
    "        try:\n",
    "            return np.array(ast.literal_eval(embedding))\n",
    "        except (ValueError, SyntaxError):\n",
    "            return None\n",
    "    elif isinstance(embedding, (list, np.ndarray)):\n",
    "        return np.array(embedding)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Function created to apply the converting funtion\n",
    "def apply_convert_embedding(df, columns):\n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(convert_embedding)\n",
    "\n",
    "\n",
    "# Function defined to utilize sklearn's cosine similarity function\n",
    "def compute_cosine_similarity(embedding1, embedding2):\n",
    "    # Only apply the function if the embedding is not size =0 or None\n",
    "    if embedding1 is None or embedding2 is None or embedding1.size == 0 or embedding2.size == 0:\n",
    "        return None\n",
    "    similarity = cosine_similarity(embedding1.reshape(1, -1), embedding2.reshape(1, -1))\n",
    "    return similarity[0][0]\n",
    "\n",
    "# Define the embeddings to use\n",
    "embedding_types = ['pii_redact_embedding', 'key_themes_embedding']\n",
    "\n",
    "# Loop through the embedding types = pii_redact_embeddings and key_themes_embeddings\n",
    "for embedding_type in embedding_types:\n",
    "    # Loop through the executive, agent, and client summaries\n",
    "    for summary in summaries:\n",
    "        #Define column name for cosine column\n",
    "        cosine_similarity_col = f'{summary}_{embedding_type}_cosine_similarity'\n",
    "        # Use the summary_embeddings_columns dictionary to retrieve the correct column name\n",
    "        summary_embedding_col = summary_embedding_columns[summary]\n",
    "\n",
    "        if summary == 'executive_summary':\n",
    "            # If the summary type is executive summary - apply the cosine similarity function to all of the rows comparing the 2 embeddings\n",
    "            public_ai_commentary[cosine_similarity_col] = public_ai_commentary.apply(\n",
    "                lambda row: compute_cosine_similarity(row[embedding_type], row[summary_embedding_col]),\n",
    "                axis=1\n",
    "            )\n",
    "        else:\n",
    "            # Identify assessment types associated with the current summary ('agency_summary' or 'client_summary')\n",
    "            relevant_assessmenttypes = [\n",
    "                atype for atype, summaries_list in assessmenttype_to_summary.items() if summary in summaries_list\n",
    "            ]\n",
    "             # Compute cosine similarity only for rows with relevant assessment types; set others to None\n",
    "            public_ai_commentary[cosine_similarity_col] = public_ai_commentary.apply(\n",
    "                lambda row: compute_cosine_similarity(row[embedding_type], row[summary_embedding_col])\n",
    "                if row['assessmenttype'] in relevant_assessmenttypes else None,\n",
    "                axis=1\n",
    "            )\n",
    "# Loop through the embedding types = pii_redact_embeddings and key_themes_embeddings\n",
    "for embedding_type in embedding_types:\n",
    "    # Loop through the executive, agent, and client summaries\n",
    "    for summary in summaries:\n",
    "        # Create column name for the cosine similarity column\n",
    "        cosine_similarity_col = f'{summary}_{embedding_type}_cosine_similarity'\n",
    "\n",
    "        if summary == 'executive_summary':\n",
    "            # If executive summary take the mean of the cosine similarity column as avg_similarites\n",
    "            avg_similarities = public_ai_commentary.groupby('mappedaccountid')[cosine_similarity_col].mean()\n",
    "        else:\n",
    "            # Identify assessment types associated with the current summary ('agency_summary' or 'client_summary')\n",
    "            relevant_assessmenttypes = [\n",
    "                atype for atype, summaries_list in assessmenttype_to_summary.items() if summary in summaries_list\n",
    "            ]\n",
    "            # Create filtered comments based of the the summary type\n",
    "            filtered_comments = public_ai_commentary[\n",
    "                public_ai_commentary['assessmenttype'].isin(relevant_assessmenttypes)\n",
    "            ]\n",
    "            # Take the mean of the cosine similarities based on the summary type\n",
    "            avg_similarities = filtered_comments.groupby('mappedaccountid')[cosine_similarity_col].mean()\n",
    "\n",
    "        # Add the avg_similarities value to the public_ai_summaries dataframe\n",
    "        public_ai_summaries.set_index('mappedaccountid', inplace=True)\n",
    "        public_ai_summaries[cosine_similarity_col] = avg_similarities\n",
    "        public_ai_summaries.reset_index(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ---------- METRIC: LLM as a Judge  ---------- ##\n",
    "\n",
    "# LLM as a judge is a technique where we pass in specific prompts and roles to an LLM an get specific metrics basefd on the quality of the summaries\n",
    "\n",
    "# Create the role for the LLM to follow\n",
    "llm_as_a_judge_role = \"\"\"\n",
    "You are an impartial evaluation assistant with expertise in summarizing and evaluating survey responses. Your role is to assess how accurately and effectively a given summary captures the content, meaning, and nuances of a set of survey comments. You will be provided with the original survey comments, the initial prompt used to generate the summary, and the LLM-generated summary.\n",
    "\n",
    "Your task is to evaluate the summary using a set of predefined attributes, assigning scores and justifying your evaluation for each attribute. Additionally, provide an overall score based on the individual attributes.\n",
    "\"\"\"\n",
    "# Create the prompt for the LLM. This includes describing the accuracy, relevance, coherence, information density, and overall grade of the summaries which will\n",
    "# be used as the main metrics when grading the summaries throug this approach.\n",
    "# IT also decribes the how the LLM should ouput it's answer including a score and the justification for that score\n",
    "llm_judge_prompt = \"\"\"\n",
    "You will be provided with:\n",
    "1. The original prompt used to generate the summary.\n",
    "2. The aggregated set of survey comments.\n",
    "3. The LLM-generated summary.\n",
    "\n",
    "Evaluate how well the summary represents the comments based on the following attributes. Provide a score from 0 to 1 for each attribute, where 1 is the highest quality and 0 is the lowest. Include a detailed justification for each score.\n",
    "\n",
    "Attributes:\n",
    "\n",
    "1. **Accuracy**: How closely does the summary reflect the key points, facts, and sentiments expressed in the survey comments?\n",
    "   - Scoring:\n",
    "     - 0.9 - 1.0: The summary is highly accurate, with all key points and sentiments captured correctly.\n",
    "     - 0.7 - 0.89: The summary captures most key points but has minor inaccuracies or omissions.\n",
    "     - 0.5 - 0.69: The summary contains some correct information but misses significant points or introduces inaccuracies.\n",
    "     - 0.25 - 0.49: The summary is largely inaccurate, with only a few correct points.\n",
    "     - 0 - 0.24: The summary is entirely inaccurate or misleading.\n",
    "\n",
    "2. **Relevance**: Does the summary focus on the most important points and themes present in the survey comments, without introducing unrelated information?\n",
    "   - Scoring:\n",
    "     - 0.9 - 1.0: All important points are included, and no irrelevant information is present.\n",
    "     - 0.7 - 0.89: Most important points are included, with minimal irrelevant content.\n",
    "     - 0.5 - 0.69: Some important points are included, but there is noticeable irrelevant information.\n",
    "     - 0.25 - 0.49: The summary includes mostly irrelevant information with few important points.\n",
    "     - 0 - 0.24: The summary is entirely irrelevant.\n",
    "\n",
    "3. **Coherence**: Is the summary logically structured, clear, and easy to understand?\n",
    "   - Scoring:\n",
    "     - 0.9 - 1.0: The summary is well-organized, clear, and easy to follow.\n",
    "     - 0.7 - 0.89: Generally coherent, with minor structural or clarity issues.\n",
    "     - 0.5 - 0.69: Partially coherent, with noticeable issues in structure or clarity.\n",
    "     - 0.25 - 0.49: Mostly incoherent, with significant clarity or structural problems.\n",
    "     - 0 - 0.24: Completely incoherent or confusing.\n",
    "\n",
    "4. **Information Density**: How well does the summary condense the survey comments, preserving the essential information while avoiding unnecessary verbosity?\n",
    "   - Scoring:\n",
    "     - 0.9 - 1.0: Concise and comprehensive, retaining essential information without excess verbosity.\n",
    "     - 0.7 - 0.89: Slightly verbose or slightly lacking in essential information.\n",
    "     - 0.5 - 0.69: Noticeably verbose or lacking key information.\n",
    "     - 0.25 - 0.49: Overly verbose or missing most essential points.\n",
    "     - 0 - 0.24: Fails to condense the information effectively.\n",
    "\n",
    "5. **Overall Quality**: Based on the individual attribute scores, provide an overall score for the summary's effectiveness in representing the survey comments.\n",
    "   - Scoring:\n",
    "     - 0.9 - 1.0: Excellent representation of the comments.\n",
    "     - 0.7 - 0.89: Good representation with minor issues.\n",
    "     - 0.5 - 0.69: Adequate but with several noticeable shortcomings.\n",
    "     - 0.25 - 0.49: Poor representation with significant issues.\n",
    "     - 0 - 0.24: Completely inadequate or misleading representation.\n",
    "\n",
    "Output Format:\n",
    "\n",
    "For each attribute, provide:\n",
    "- **Attribute Name:** [Score]\n",
    "- **Justification:** Explain why you assigned this score, referencing specific parts of the summary and survey comments as needed.\n",
    "\n",
    "Finally, include an **Overall Score** and an **Overall Justification** that summarizes the key points from your evaluation.\n",
    "\"\"\"\n",
    "\n",
    "# Function to generate a response from the LLM based off the prompts and roles described above\n",
    "def evaluate_summaries_with_llm(df: pd.DataFrame, summaries: list, llm_as_a_judge_role: str, llm_judge_prompt: str, client) -> pd.DataFrame:\n",
    "    for index, row in df.iterrows():\n",
    "        for summary in summaries:\n",
    "            summary_column = summary\n",
    "            prompt_column = f\"{summary}_prompt\"\n",
    "            evaluation_column = f\"{summary}_llm_rubric\"\n",
    "            summary_text = row.get(summary_column)\n",
    "            final_prompt = row.get(prompt_column)\n",
    "            if pd.isnull(final_prompt) or pd.isnull(summary_text):\n",
    "                df.at[index, evaluation_column] = None\n",
    "                continue\n",
    "            if '\\n\\n' in final_prompt:\n",
    "                original_prompt, comments_text = final_prompt.split('\\n\\n', 1)\n",
    "            else:\n",
    "                original_prompt = final_prompt\n",
    "                comments_text = ''\n",
    "            user_content = f\"\"\"\n",
    "{llm_judge_prompt}\n",
    "\n",
    "**Original Prompt:**\n",
    "{original_prompt}\n",
    "\n",
    "**Aggregated Survey Comments:**\n",
    "{comments_text}\n",
    "\n",
    "**LLM-Generated Summary:**\n",
    "{summary_text}\n",
    "\"\"\"\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": llm_as_a_judge_role},\n",
    "                {\"role\": \"user\", \"content\": user_content}\n",
    "            ]\n",
    "            try:\n",
    "                response = client.chat.completions.create(\n",
    "                    model=AZURE_OPENAI_DEPLOYMENT,\n",
    "                    messages=messages,\n",
    "                    max_tokens=800,\n",
    "                    temperature=0\n",
    "                )\n",
    "                evaluation = response.choices[0].message.content.strip()\n",
    "                df.at[index, evaluation_column] = evaluation\n",
    "            except Exception as e:\n",
    "                df.at[index, evaluation_column] = f\"Error processing: {e}\"\n",
    "    return df\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    api_version=API_VERSION,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ---------- METRIC: High Frequency Keywords  ---------- ##\n",
    "\n",
    "# High frequency key words are extracted from the comments and the summary and compared to generate a representativeness score\n",
    "\n",
    "def extract_high_frequency_keywords(comments, n_keywords=10):\n",
    "    # Initialize scikit-learn's TfidfVectorizer while removing the english stop words ('the', 'and', etc) with a maximum of 50 unique words\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_features=50)\n",
    "    # Create TF_IDF matrix which is a matrix that counts and normalizes the occurances of each word.\n",
    "    # This also incorporates Inverse Document Frequency which which is a measure of how common a word is in all documents\n",
    "    tfidf_matrix = vectorizer.fit_transform(comments)\n",
    "    # Sum the tfidf score across all comments to get a total score\n",
    "    sum_tfidf = tfidf_matrix.sum(axis=0).A1\n",
    "    # Retrieve the feature names corresponding to each score\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    # Create a dictionary pairing the terms with their corrseponding total score\n",
    "    keywords_scores = list(zip(terms, sum_tfidf))\n",
    "    # Sort the list by score and select the term without it's corresponding score\n",
    "    keywords_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    # Extract the top n_keywords. In this case 10 key words\n",
    "    high_frequency_keywords = [term for term, score in keywords_scores[:n_keywords]]\n",
    "\n",
    "    return high_frequency_keywords\n",
    "\n",
    "# Function that creates a score by dividing the number of common keywords in the summary and comments by the number of high frequency key words\n",
    "def calculate_representativeness_score(summary, high_frequency_keywords):\n",
    "    if isinstance(summary, str):\n",
    "        summary_lower = summary.lower()\n",
    "        # Create a list that contains any words in the summary that were included in the high frequency key words\n",
    "        keywords_in_summary = [kw for kw in high_frequency_keywords if kw in summary_lower]\n",
    "        # Calculate score based off the number of similar keywords divided by the number of high frequency key words\n",
    "        representativeness_score = len(keywords_in_summary) / len(high_frequency_keywords) if high_frequency_keywords else 0\n",
    "    else:\n",
    "        keywords_in_summary = []\n",
    "        representativeness_score = 0\n",
    "    # Return both the keywords and the calculated score\n",
    "    return keywords_in_summary, representativeness_score\n",
    "\n",
    "\n",
    "# Function that groups the comments by there summary type - executive, agancy, or client\n",
    "def create_comment_groups(df, text_column, group_by_column, assessment_column):\n",
    "    # Initiailize dictionary to hold the comments\n",
    "    comment_groups = {}\n",
    "\n",
    "    # Loop through both the key and the value of summary types\n",
    "    for summary_type, assessment_filter in summary_types.items():\n",
    "        # If agency or client summary - create a new dataframe that only contains values where the assesment type matches the asessment filter\n",
    "        if assessment_filter:\n",
    "            filtered_df = df[df[assessment_column].isin(assessment_filter)]\n",
    "        else:\n",
    "            # Executive dataframe contains both the agency and client summaries so we copy the etire datatframe rather than filtering\n",
    "            filtered_df = df.copy()\n",
    "        # Group by the groupp_by_column (mappedacountid in this case) and extract the text as a list\n",
    "        groups = filtered_df.groupby(group_by_column)[text_column].apply(list)\n",
    "        # Add the text to the comment groups dictionary\n",
    "        comment_groups[summary_type] = groups.to_dict()\n",
    "\n",
    "    # Return the comments group dictionary\n",
    "    return comment_groups\n",
    "\n",
    "# Function to add high frequency key words, keywords in summary, and representativeness score to public_ai_summmaries df\n",
    "def frequency_based_keyword_analysis(public_ai_summaries, comment_groups, summary_column):\n",
    "    for col_suffix in ['high_frequency_keywords', 'keywords_in_summary', 'representativeness_score']:\n",
    "        # Create column name from the list defined above\n",
    "        col_name = f\"{summary_column}_{col_suffix}\"\n",
    "        if col_name not in public_ai_summaries.columns:\n",
    "            public_ai_summaries[col_name] = None\n",
    "\n",
    "    # Loop through key and values of comment_groups dictionary\n",
    "    for group_id, comments in comment_groups.items():\n",
    "        # Extract the keywords for the comment group you are looping through\n",
    "        high_frequency_keywords = extract_high_frequency_keywords(comments)\n",
    "\n",
    "        # Create a summary by filtering from the mask that was created above, which filter by current group\n",
    "        summary = public_ai_summaries.loc[mask, summary_column].iloc[0]\n",
    "        # Extract the key words in the summary and calcualate the Representativeness score from the filtered summary along iwth the high frequency score\n",
    "        keywords_in_summary, representativeness_score = calculate_representativeness_score(summary, high_frequency_keywords)\n",
    "\n",
    "        # Identify the indexes that the mask created for each filter\n",
    "        index_to_update = public_ai_summaries.loc[mask].index[0]\n",
    "\n",
    "        # Update the public summaries dataframe according to the mask and the corresponding words/scores for each group\n",
    "        public_ai_summaries.at[index_to_update, f\"{summary_column}_high_frequency_keywords\"] = high_frequency_keywords\n",
    "        public_ai_summaries.at[index_to_update, f\"{summary_column}_keywords_in_summary\"] = keywords_in_summary\n",
    "        public_ai_summaries.at[index_to_update, f\"{summary_column}_representativeness_score\"] = representativeness_score\n",
    "\n",
    "    return public_ai_summaries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ---------- METRIC: Rouge Scores  ---------- ##\n",
    "\n",
    "# Ruoge Scores are an evaluation score used to measure how often a word or group of words from a summary appear in the text it was summarizing.\n",
    "# Rouge1 takes into account 1 word, Rouge2 takes into account pairs of words, and Rouge L focuses on the longest common subsequences\n",
    "\n",
    "# Function that concatenates the comments based on the group id\n",
    "def concatenate_comments(comment_groups):\n",
    "    concatenated_texts = {group_id: ' '.join(comments) for group_id, comments in comment_groups.items()}\n",
    "    return concatenated_texts\n",
    "\n",
    "# Function that uses the imported RougeScorer Function to Calculate the 3 Rouge scores\n",
    "def calculate_rouge_scores(public_ai_summaries, comment_groups):\n",
    "    # Initialize the Rouge Scorer\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    # Define the summary types\n",
    "    for summary_type in summary_types:\n",
    "        # Loop through the summary types and concatentate the comments\n",
    "        concatenated_texts = concatenate_comments(comment_groups[summary_type])\n",
    "\n",
    "        for col_suffix in ['rouge_1', 'rouge_2', 'rouge_L', 'comment_group']:\n",
    "            # Define the column name\n",
    "            col_name = f\"{summary_type}_{col_suffix}\"\n",
    "            if col_name not in public_ai_summaries.columns:\n",
    "                if 'rouge' in col_suffix:\n",
    "                    public_ai_summaries[col_name] = np.nan\n",
    "                else:\n",
    "                    public_ai_summaries[col_name] = None\n",
    "\n",
    "        # Find the indices where the mappedaccountid is include in the concatenated_texts dictionary\n",
    "        valid_indices = public_ai_summaries['mappedaccountid'].isin(concatenated_texts.keys())\n",
    "        # Filter to only the valid rows\n",
    "        valid_rows = public_ai_summaries.loc[valid_indices]\n",
    "\n",
    "        for index, row in valid_rows.iterrows():\n",
    "            # Define the group_id, the summary text, and the referenct text\n",
    "            group_id = row['mappedaccountid']\n",
    "            summary_text = row[summary_type]\n",
    "            reference_text = concatenated_texts[group_id]\n",
    "\n",
    "            # Assign the reference text value to the public_ai_summaries at the specific index\n",
    "            public_ai_summaries.at[index, f\"{summary_type}_comment_group\"] = reference_text\n",
    "\n",
    "            if not isinstance(reference_text, str) or not reference_text.strip():\n",
    "                continue\n",
    "            if not isinstance(summary_text, str) or not summary_text.strip():\n",
    "                continue\n",
    "\n",
    "            # Perform the Rouge scores from the reference text compared to the summary text\n",
    "            scores = scorer.score(reference_text, summary_text)\n",
    "            # Record the rouge1, rouge2, and rougeL scores\n",
    "            public_ai_summaries.at[index, f\"{summary_type}_rouge_1\"] = scores['rouge1'].fmeasure\n",
    "            public_ai_summaries.at[index, f\"{summary_type}_rouge_2\"] = scores['rouge2'].fmeasure\n",
    "            public_ai_summaries.at[index, f\"{summary_type}_rouge_L\"] = scores['rougeL'].fmeasure\n",
    "\n",
    "    return public_ai_summaries\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ---------- METRIC: K Means Clustering  ---------- ##\n",
    "\n",
    "# Here we are clustering the comments into 3 groups usink k-means clusters and then extracting the top 5 key words from each cluster\n",
    "\n",
    "# Convert embedding to numpy array\n",
    "def convert_embedding(embedding):\n",
    "    if isinstance(embedding, str):\n",
    "        try:\n",
    "            return np.array(ast.literal_eval(embedding))\n",
    "        except (ValueError, SyntaxError):\n",
    "            return None\n",
    "    elif isinstance(embedding, (list, np.ndarray)):\n",
    "        return np.array(embedding)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_high_frequency_keywords(comments, n_keywords=5):\n",
    "    # Initialize scikit-learn's TfidfVectorizer while removing the english stop words ('the', 'and', etc) with a maximum of 50 unique words\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_features=50)\n",
    "    # Create TF_IDF matrix which is a matrix that counts and normalizes the occurances of each word.\n",
    "    # This also incorporates Inverse Document Frequency which which is a measure of how common a word is in all documents\n",
    "    tfidf_matrix = vectorizer.fit_transform(comments)\n",
    "    # Sum the tfidf score across all comments to get a total score\n",
    "    sum_tfidf = tfidf_matrix.sum(axis=0).A1\n",
    "    # Retrieve the feature names corresponding to the scores\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    # Create a dictionary pairing the terms with their corrseponding total score\n",
    "    keywords_scores = list(zip(terms, sum_tfidf))\n",
    "    # Sort the list by score and select the term without it's corresponding score\n",
    "    keywords_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    # Extract the top n_keywords. In this case 5 key words\n",
    "    high_frequency_keywords = [term for term, score in keywords_scores[:n_keywords]]\n",
    "\n",
    "    return high_frequency_keywords\n",
    "\n",
    "def perform_topic_modeling(public_ai_commentary, public_ai_summaries):\n",
    "\n",
    "    # Define embedding variables that relate to the commentary and the summaries\n",
    "    embedding_columns_commentary = ['pii_redact_embedding']\n",
    "\n",
    "    # Convert embeddings to numpy arrays\n",
    "    for col in embedding_columns_commentary:\n",
    "        if col in public_ai_commentary.columns:\n",
    "            public_ai_commentary[col] = public_ai_commentary[col].apply(convert_embedding)\n",
    "\n",
    "    for col in embedding_columns_summaries:\n",
    "        if col in public_ai_summaries.columns:\n",
    "            public_ai_summaries[col] = public_ai_summaries[col].apply(convert_embedding)\n",
    "\n",
    "\n",
    "    for summary_type in summary_types.keys():\n",
    "        if f'{summary_type}_topics' not in public_ai_summaries.columns:\n",
    "            public_ai_summaries[f'{summary_type}_topics'] = None\n",
    "        if f'{summary_type}_topic_similarity' not in public_ai_summaries.columns:\n",
    "            public_ai_summaries[f'{summary_type}_topic_similarity'] = np.nan\n",
    "\n",
    "    # Loop through the key and value in the summary_type dictionary\n",
    "    for summary_type, assessment_filter in summary_types.items():\n",
    "        # Group the commentary by mappedaccountid\n",
    "        grouped = public_ai_commentary.groupby('mappedaccountid')\n",
    "        # Loop through each df grouped by mappedaccountid\n",
    "        for mappedaccountid, group in grouped:\n",
    "            # Filter comments based on assessmenttype\n",
    "            if group.empty:\n",
    "                continue\n",
    "            # Create list of comment embeddings from the PII comment embeddings\n",
    "            comment_embeddings = group['pii_redact_embedding'].dropna().tolist()\n",
    "            if len(comment_embeddings) == 0:\n",
    "                continue\n",
    "            # Convert embeddings to a numpy vertical stack\n",
    "            X = np.vstack(comment_embeddings)\n",
    "            # Define the number of clusters to be 3 or the number of embeddings, whichever is less\n",
    "            n_clusters = min(3, len(comment_embeddings))\n",
    "            if n_clusters < 1:\n",
    "                continue\n",
    "            # Perform K-means cluster on the embeddings\n",
    "            kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "            kmeans.fit(X)\n",
    "            # The centroids represent the center of the topic embeddings\n",
    "            centroids = kmeans.cluster_centers_\n",
    "            # Define the summary embedding column name\n",
    "            summary_embedding_col = f'{summary_type}_embedding'\n",
    "            if summary_embedding_col not in public_ai_summaries.columns:\n",
    "                continue\n",
    "            # Filter the dataframe to inclcude only the current mappedaccount id and the summary embedding column\n",
    "            summary_embedding_series = public_ai_summaries.loc[public_ai_summaries['mappedaccountid'] == mappedaccountid, summary_embedding_col]\n",
    "            if summary_embedding_series.empty or not isinstance(summary_embedding_series.iloc[0], np.ndarray):\n",
    "                continue\n",
    "            # Take the first element from the summary embedding column\n",
    "            summary_embedding = summary_embedding_series.iloc[0]\n",
    "            # Perform cosine similarity comparing the summary embedding column to the centroids\n",
    "            similarities = cosine_similarity([summary_embedding], centroids)[0]\n",
    "            avg_similarity = np.mean(similarities)\n",
    "            cluster_labels = kmeans.labels_\n",
    "            topics = {}\n",
    "            for label in np.unique(cluster_labels):\n",
    "                cluster_comments = group.iloc[np.where(cluster_labels == label)[0]]['pii_redact'].dropna().tolist()\n",
    "                high_frequency_keywords = extract_high_frequency_keywords(cluster_comments, n_keywords=5)\n",
    "                topics[f'Cluster_{label}'] = high_frequency_keywords\n",
    "            idx = public_ai_summaries.loc[public_ai_summaries['mappedaccountid'] == mappedaccountid].index\n",
    "            if len(idx) == 0:\n",
    "                continue\n",
    "            idx = idx[0]\n",
    "            public_ai_summaries.at[idx, f'{summary_type}_topics'] = topics\n",
    "            public_ai_summaries.at[idx, f'{summary_type}_topic_similarity'] = avg_similarity\n",
    "\n",
    "    return public_ai_summaries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ---------- METRIC: Pairwise Cosine Similarity (Diversity Score Only) ---------- ##\n",
    "\n",
    "# This metric is meant to use cosine similarity to compare the simlilarity between the comments and create a diversity score\n",
    "\n",
    "# Function to convert the embeddings to numpy arrays\n",
    "def convert_embedding(embedding):\n",
    "    if isinstance(embedding, str):\n",
    "        try:\n",
    "            return np.array(ast.literal_eval(embedding))\n",
    "        except (ValueError, SyntaxError):\n",
    "            return None\n",
    "    elif isinstance(embedding, (list, np.ndarray)):\n",
    "        return np.array(embedding)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Function to calculate the diversity score between the comments\n",
    "def calculate_diversity(public_ai_commentary, public_ai_summaries):\n",
    "    embedding_columns_commentary = ['pii_redact_embedding']\n",
    "\n",
    "    # Convert the embeddings column to numpy array\n",
    "    for col in embedding_columns_commentary:\n",
    "        if col in public_ai_commentary.columns:\n",
    "            public_ai_commentary[col] = public_ai_commentary[col].apply(convert_embedding)\n",
    "\n",
    "    # Define the summary types and their related fields\n",
    "\n",
    "    for summary_type in summary_types.keys():\n",
    "        diversity_col = f'{summary_type}_diversity_score'\n",
    "        if diversity_col not in public_ai_summaries.columns:\n",
    "            public_ai_summaries[diversity_col] = np.nan\n",
    "\n",
    "\n",
    "    for summary_type, assessment_filter in summary_types.items():\n",
    "        # group the data by mappedaccountid\n",
    "        grouped = public_ai_commentary.groupby('mappedaccountid')\n",
    "        for mappedaccountid, group in grouped:\n",
    "            if assessment_filter:\n",
    "                group = group[group['assessmenttype'].isin(assessment_filter)]\n",
    "            if group.empty:\n",
    "                continue\n",
    "            # Create embeddings list filtered on groupid\n",
    "            comment_embeddings = group['pii_redact_embedding'].dropna().tolist()\n",
    "            if len(comment_embeddings) < 2:\n",
    "                continue\n",
    "            # Creat numpy verticle array from embeddings list\n",
    "            X = np.vstack(comment_embeddings)\n",
    "\n",
    "            # Create cosine similarity matrix from the cosine_similarity function\n",
    "            similarity_matrix = cosine_similarity(X)\n",
    "\n",
    "            # Calculate n from the number of rows in the matrix\n",
    "            n = similarity_matrix.shape[0]\n",
    "\n",
    "            # Take the sum of the similarity matrix, and subtract n which acounts for the number of 1 elements that would be contained in the daiganol row\n",
    "            sum_similarities = np.sum(similarity_matrix) - n\n",
    "            # Determine the count by multiplying the number of rows by the number of columns - 1. This again acccounts for the diaganol row\n",
    "            count = n * (n - 1)\n",
    "            # Calcualte the average\n",
    "            average_pairwise_similarity = sum_similarities / count\n",
    "            # Calcualte the diversity score by subtracting the average from 1\n",
    "            diversity_score = 1 - average_pairwise_similarity\n",
    "\n",
    "            # Determine the index of the rows for the specific mappedaccountid\n",
    "            idx = public_ai_summaries.loc[public_ai_summaries['mappedaccountid'] == mappedaccountid].index\n",
    "            if len(idx) == 0:\n",
    "                continue\n",
    "            idx = idx[0]\n",
    "            # Name the diversity column and add the diversity score\n",
    "            diversity_col = f'{summary_type}_diversity_score'\n",
    "            public_ai_summaries.at[idx, diversity_col] = diversity_score\n",
    "\n",
    "    return public_ai_summaries\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DaveShevy-Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
